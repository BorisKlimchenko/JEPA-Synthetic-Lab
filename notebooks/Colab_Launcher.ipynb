{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6711d959",
      "metadata": {
        "id": "6711d959"
      },
      "outputs": [],
      "source": [
        "# @title 1. Initialize & Hardware Check\n",
        "import os\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Clone/Update Repo\n",
        "REPO_URL = \"https://github.com/BorisKlimchenko/Adaptive-Motion-Lab.git\"\n",
        "REPO_DIR = \"/content/Adaptive-Motion-Lab\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(f\"Cloning {REPO_URL}...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(\"Pulling latest changes...\")\n",
        "    !cd {REPO_DIR} && git pull\n",
        "\n",
        "# 3. Hardware Check\n",
        "print(\"\\n--- Hardware Profile ---\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU: {gpu}\")\n",
        "    if \"T4\" in gpu: print(\"‚ÑπÔ∏è Mode: Efficiency (T4)\")\n",
        "    elif \"A100\" in gpu: print(\"üöÄ Mode: Performance (A100)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected!\")\n",
        "\n",
        "%cd {REPO_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232c8cd4",
      "metadata": {
        "id": "232c8cd4"
      },
      "outputs": [],
      "source": [
        "# @title 2. Install Dependencies\n",
        "print(\"Installing requirements...\")\n",
        "!pip install -r requirements.txt\n",
        "!pip install xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu121\n",
        "print(\"‚úÖ Ready to run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82feced2",
      "metadata": {
        "id": "82feced2"
      },
      "outputs": [],
      "source": [
        "# @title 3. Run Inference Engine\n",
        "# –ó–¥–µ—Å—å –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ç–æ–ª—å–∫–æ —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–ª–∏!\n",
        "CONFIG_PATH = \"configs/default_scene.json\"\n",
        "\n",
        "print(f\"üöÄ Launching Engine with: {CONFIG_PATH}\")\n",
        "!python main.py --prompts {CONFIG_PATH}"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}